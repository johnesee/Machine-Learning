{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks using GPUs\n",
    "*by Michael Johnesee, December 2017*\n",
    "*johnesee@rams.colostate.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The advantages of using a Graphical Processing Unit (GPU) for Neural Networks are power efficiency and improved speed. Speedup comes from the inherently parallelizable nature of Neural Networks and the superior floating point operations of GPUs. Multiplications within each layer can be computed independently. Data dependencies are only found between layers. Latency-insensitive inference tasks during the training run can be pipelined between layers.  Batch training also benefits from using GPUs as well. This notebook looks into the various techniques for GPU utilization and features a simple speed for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "###     Research\n",
    "\n",
    "Individually, GPUs feature many advantages over CPUs for parallel tasks. They typically have more computational units and a higher bandwidth for memory retrieval. Their architecture is ideal for neural network computations. Since they were designed for image processing, they also can be exploited for further speed-up.\n",
    "\n",
    "In addtion, there are several techniques for parallelizing a neural network using multiple GPUs:\n",
    "* Data Parallelism - Each GPU receives a complete copy of the model and trains it on a fraction of the data.\n",
    "* Model Parallelism - Network Model is split by assigning a layer to each GPU, with no GPU needing to maintain all the parameters .  Layers are connected by output activation order.\n",
    "* Hyper-Parameter Parallelism - Multiple training runs of different neural network models.\n",
    "\n",
    "Combining these techniques is surprisingly simple, with 16 GPUs being utilized through splitting a model over 4 GPUs and performing data parallelism on 4 copies on that partition.  A different Neural Network model could then be run on another 16 GPUs.\n",
    "\n",
    "###     Implementation\n",
    "Since access to multiple GPUs is rare, the code below looks at the time improvement that a single GPU has over a CPU. The data below was gathered on 'Swordfish', located in the Computer Science Building of Colorado State University. \n",
    "\n",
    "PyTorch is a python package that provides two high-level features:\n",
    "* Tensor computation (like numpy) with strong GPU acceleration\n",
    "* Deep Neural Networks built on a tape-based autograd system\n",
    "\n",
    "The code is based on an example provided by PyTorch: \n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "to train on the popular MNIST database of handwritten digits:\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "PyTorch operates on Linux or Mac OS and has built-in CUDA support for nVidia GPUs. It is recommended to install using Anaconda Package Manager:\n",
    "\n",
    "*conda install pytorch torchvision -c pytorch*\n",
    "\n",
    "### [Jump to Conclusion](#the_destination)\n",
    "\n",
    "## CPU Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies:\n",
    "torch: \n",
    "* Tensor library with strong GPU support\n",
    "\n",
    "torchvision: \n",
    "* image and video datasets and models for torch deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters:\n",
    "\n",
    "Batch Size: \n",
    "* the number of training examples in one forward/backward pass. The higher the batch size, the more memory space required.\n",
    "\n",
    "Epoch: \n",
    "* one forward pass and one backward pass of all the training examples\n",
    "\n",
    "Momentum: \n",
    "* make convergence faster with gradient descent vs a constant step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Function Choices\n",
    "Loss Function: Negative Log-Likelihood\n",
    "* L(y) = −log(y)\n",
    "* NLL is seful to train classification problems\n",
    "\n",
    "Weight Function: Stochastic Gradient Descent\n",
    "* weight = weight - learning_rate * gradient\n",
    "* SGD is used to minimize the high cost of running back propagation over the full training set.\n",
    "\n",
    "Both torch.optim and torch.nn.functional feature many other predefined functions to optimize their neural network code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "kwargs = {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True, \n",
    "                                                          transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                                                                        transforms.Normalize((0.1307,),\n",
    "                                                                                                             (0.3081,))])), \n",
    "                                           batch_size=train_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', \n",
    "                                                         train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                    transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                          batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "test_model = Net()\n",
    "model = test_model\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Outputs:\n",
    "* train: Periodically prints the loss value, which is an estimate of how far away the output is from the target.\n",
    "* test: Prints the average loss and how accurate the test data is to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and epoch % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.323592\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.185648\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.223868\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.356802\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.420115\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.110812\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.149399\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.148355\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.193203\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.112662\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.074151\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.077076\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.102261\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.018784\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.228549\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.171667\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.057220\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.043385\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.030104\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.076178\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.191099\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.227663\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.176275\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.112615\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.102808\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.074247\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.039808\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.169042\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.091818\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.065232\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.496620\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.125451\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.198283\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.181798\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.049775\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.046217\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.273475\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.066713\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.163061\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.185408\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.095304\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 0.049429\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.192918\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.040477\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.088771\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.062795\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 0.169387\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.273955\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.140255\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.081675\n",
      "\n",
      "Test set: Average loss: 0.0281, Accuracy: 9907/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_times = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    current_time = time.time()\n",
    "    train(epoch)\n",
    "    epoch_times.append(time.time() -current_time)\n",
    "\n",
    "current_time = time.time()\n",
    "test()\n",
    "test_time=time.time()-current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(seed)\n",
    "kwargs_cuda = {'num_workers': 0, 'pin_memory': True}\n",
    "\n",
    "train_loader_cuda = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True, \n",
    "                                                          transform=transforms.Compose([transforms.ToTensor(), \n",
    "                                                                                        transforms.Normalize((0.1307,),\n",
    "                                                                                                             (0.3081,))])), \n",
    "                                           batch_size=train_batch_size, shuffle=True, **kwargs_cuda)\n",
    "test_loader_cuda = torch.utils.data.DataLoader(datasets.MNIST('../data', \n",
    "                                                         train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                    transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                          batch_size=test_batch_size, shuffle=True, **kwargs_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Important!*\n",
    "Call to .cuda() could take several minutes depending on setup (newer Pascal Architecture). Update with below command to fix current binary:\n",
    "\n",
    "*conda install pytorch torchvision cuda80 -c soumith*\n",
    "\n",
    "Moving a model to a GPU via .cuda() must be done before constructing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cuda = test_model.cuda()\n",
    "optimizer_cuda = optim.SGD(model_cuda.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cuda(epoch):\n",
    "    model_cuda.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_cuda):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer_cuda.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_cuda.step()\n",
    "        if batch_idx % log_interval == 0 and epoch % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_cuda.dataset),\n",
    "                100. * batch_idx / len(train_loader_cuda), loss.data[0]))\n",
    "\n",
    "\n",
    "def test_cuda():\n",
    "    model_cuda.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model_cuda(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader_cuda.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader_cuda.dataset),\n",
    "        100. * correct / len(test_loader_cuda.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.112951\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.115494\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.159213\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.215078\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.324018\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.071389\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.060406\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.076174\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.202793\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.129243\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.082932\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.046993\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.076732\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.109644\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.076831\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.052845\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.048926\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.105494\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.083646\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.055619\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.150667\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.108131\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.038810\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.043779\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.031034\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.075573\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.049737\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.087610\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.078228\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.007483\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.113177\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.093518\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.114508\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.055749\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.035101\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.101688\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.062324\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.031311\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.041255\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.038991\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.059148\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 0.160245\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.008526\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.052538\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.019828\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.045642\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 0.147717\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.070950\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.029240\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.053685\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9910/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_times_cuda = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    current_time = time.time()\n",
    "    train_cuda(epoch)\n",
    "    epoch_times_cuda.append(time.time() - current_time)\n",
    "    \n",
    "current_time = time.time()\n",
    "test_cuda()\n",
    "test_time_cuda=time.time()- current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>\n",
    "## Conclusion\n",
    "Due to Neural Networks overtaking traditional computational models, they will be a driving force in most industries. By utilising GPUs, every implementation will benefit from improved execution time, reduced energy consumption, and lowered costs. This will also allow for more complex networks and lead to more accurate solutions.\n",
    "### Results\n",
    "Both executions achieved an accuracy score of 99% and a average loss less than 0.03. However, as seen in the graphs below, the GPU showed a significantly reduced execution time in all areas; training, testing, and overall time. By using a GPU in this simple example, we achieved a speed-up over five times faster than the CPU time. The GPU also had more consistent epoch times recorded, with a range of one compared to the eighteen of the CPU. With the extra time, a GPU implemented network could have trained over more iterations or with a larger data set. A completely different network could have also run after the GPU execution finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIKOQYCIqKQYyAgIgo5BgIiopD7/yRNBAsaA/M3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7bebe4048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title('Epoch Times')\n",
    "plt.plot(epoch_times, marker='o', color='b', label='CPU')\n",
    "plt.plot(epoch_times_cuda, marker='s', color='g', label='GPU')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7b805ee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_groups = 3\n",
    "means_cpu = (np.mean(epoch_times), test_time, np.sum(epoch_times)+test_time)\n",
    "means_gpu = (np.mean(epoch_times_cuda), test_time_cuda, np.sum(epoch_times_cuda)+test_time_cuda)\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "opacity = 0.8\n",
    "rects1 = plt.bar(index, means_cpu, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 label='CPU')\n",
    "rects2 = plt.bar(index + bar_width, means_gpu, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='g',\n",
    "                 label='GPU')\n",
    "plt.title('Execution Time')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xticks(index + bar_width, ('Mean Epoc', 'Test Time', 'Total Time'))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Campos, Víctor, et al. “Distributed Training Strategies for a Computer Vision Deep Learning Algorithm on a Distributed GPU Cluster.” Procedia Computer Science, vol. 108, 2017, pp. 315–324., doi:10.1016/j.procs.2017.05.074.\n",
    "\n",
    "Li Deng, J., et al. “Scalable Stacking and Learning for Building Deep Architectures.”Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference On, 2012, pp. 2133–2136.\n",
    "\n",
    "Zhang, Qingchen, et al. “A Survey on Deep Learning for Big Data.” Information Fusion, vol. 42, 2018, pp. 146–157., doi:10.1016/j.inffus.2017.10.006.\n",
    "\n",
    "Dettmers, Tim. How to Parallelize Deep Learning on GPUs Part 1/2: Data Parallelism. 10 Sept. 2014, timdettmers.com/2014/10/09/deep-learning-data-parallelism/.\n",
    "\n",
    "“Welcome to PyTorch Tutorials.” PyTorch Tutorials, Pytorch, pytorch.org/tutorials/index.html. \n",
    "\n",
    "“Training with Multiple GPUs Using Model Parallelism” MXNet How To, Apache Incubator, mxnet.incubator.apache.org/how_to/model_parallel_lstm.html. \n",
    "\n",
    "“Run MXNet on Multiple CPU/GPUs with Data Parallelism” MXNet How To, Apache Incubator, mxnet.incubator.apache.org/how_to/multi_devices.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
